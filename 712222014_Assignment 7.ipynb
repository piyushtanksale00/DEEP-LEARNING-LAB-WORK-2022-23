{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012cf7ac-bc59-434f-b566-7b890815139b",
   "metadata": {},
   "source": [
    "RUSHIKESH SANJIV TANKSALE.\n",
    "712222014 Deep Learning 2022-23 SEM II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "768a8aef-ecde-4572-8dfb-29ee3a345eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.382190\n",
      "Epoch 2/50, Loss: 0.379003\n",
      "Epoch 3/50, Loss: 0.376091\n",
      "Epoch 4/50, Loss: 0.373415\n",
      "Epoch 5/50, Loss: 0.370942\n",
      "Epoch 6/50, Loss: 0.368642\n",
      "Epoch 7/50, Loss: 0.366492\n",
      "Epoch 8/50, Loss: 0.364468\n",
      "Epoch 9/50, Loss: 0.362555\n",
      "Epoch 10/50, Loss: 0.360736\n",
      "Epoch 11/50, Loss: 0.358999\n",
      "Epoch 12/50, Loss: 0.357334\n",
      "Epoch 13/50, Loss: 0.355732\n",
      "Epoch 14/50, Loss: 0.354185\n",
      "Epoch 15/50, Loss: 0.352688\n",
      "Epoch 16/50, Loss: 0.351236\n",
      "Epoch 17/50, Loss: 0.349824\n",
      "Epoch 18/50, Loss: 0.348449\n",
      "Epoch 19/50, Loss: 0.347108\n",
      "Epoch 20/50, Loss: 0.345799\n",
      "Epoch 21/50, Loss: 0.344519\n",
      "Epoch 22/50, Loss: 0.343267\n",
      "Epoch 23/50, Loss: 0.342042\n",
      "Epoch 24/50, Loss: 0.340841\n",
      "Epoch 25/50, Loss: 0.339664\n",
      "Epoch 26/50, Loss: 0.338509\n",
      "Epoch 27/50, Loss: 0.337377\n",
      "Epoch 28/50, Loss: 0.336265\n",
      "Epoch 29/50, Loss: 0.335173\n",
      "Epoch 30/50, Loss: 0.334101\n",
      "Epoch 31/50, Loss: 0.333048\n",
      "Epoch 32/50, Loss: 0.332013\n",
      "Epoch 33/50, Loss: 0.330996\n",
      "Epoch 34/50, Loss: 0.329996\n",
      "Epoch 35/50, Loss: 0.329012\n",
      "Epoch 36/50, Loss: 0.328045\n",
      "Epoch 37/50, Loss: 0.327093\n",
      "Epoch 38/50, Loss: 0.326156\n",
      "Epoch 39/50, Loss: 0.325234\n",
      "Epoch 40/50, Loss: 0.324327\n",
      "Epoch 41/50, Loss: 0.323433\n",
      "Epoch 42/50, Loss: 0.322553\n",
      "Epoch 43/50, Loss: 0.321687\n",
      "Epoch 44/50, Loss: 0.320833\n",
      "Epoch 45/50, Loss: 0.319992\n",
      "Epoch 46/50, Loss: 0.319162\n",
      "Epoch 47/50, Loss: 0.318345\n",
      "Epoch 48/50, Loss: 0.317540\n",
      "Epoch 49/50, Loss: 0.316745\n",
      "Epoch 50/50, Loss: 0.315962\n",
      "Optimizer: gd, Accuracy: 0.7000\n",
      "Epoch 1/50, Loss: 0.625325\n",
      "Epoch 2/50, Loss: 0.597084\n",
      "Epoch 3/50, Loss: 0.571384\n",
      "Epoch 4/50, Loss: 0.548504\n",
      "Epoch 5/50, Loss: 0.528083\n",
      "Epoch 6/50, Loss: 0.510581\n",
      "Epoch 7/50, Loss: 0.494975\n",
      "Epoch 8/50, Loss: 0.481549\n",
      "Epoch 9/50, Loss: 0.470050\n",
      "Epoch 10/50, Loss: 0.459761\n",
      "Epoch 11/50, Loss: 0.450535\n",
      "Epoch 12/50, Loss: 0.442677\n",
      "Epoch 13/50, Loss: 0.435668\n",
      "Epoch 14/50, Loss: 0.429448\n",
      "Epoch 15/50, Loss: 0.424285\n",
      "Epoch 16/50, Loss: 0.419413\n",
      "Epoch 17/50, Loss: 0.415290\n",
      "Epoch 18/50, Loss: 0.411393\n",
      "Epoch 19/50, Loss: 0.407918\n",
      "Epoch 20/50, Loss: 0.404807\n",
      "Epoch 21/50, Loss: 0.402281\n",
      "Epoch 22/50, Loss: 0.399857\n",
      "Epoch 23/50, Loss: 0.397664\n",
      "Epoch 24/50, Loss: 0.395756\n",
      "Epoch 25/50, Loss: 0.393945\n",
      "Epoch 26/50, Loss: 0.392252\n",
      "Epoch 27/50, Loss: 0.390741\n",
      "Epoch 28/50, Loss: 0.389342\n",
      "Epoch 29/50, Loss: 0.388003\n",
      "Epoch 30/50, Loss: 0.386845\n",
      "Epoch 31/50, Loss: 0.385777\n",
      "Epoch 32/50, Loss: 0.384713\n",
      "Epoch 33/50, Loss: 0.383702\n",
      "Epoch 34/50, Loss: 0.382743\n",
      "Epoch 35/50, Loss: 0.381937\n",
      "Epoch 36/50, Loss: 0.381217\n",
      "Epoch 37/50, Loss: 0.380444\n",
      "Epoch 38/50, Loss: 0.379726\n",
      "Epoch 39/50, Loss: 0.379026\n",
      "Epoch 40/50, Loss: 0.378361\n",
      "Epoch 41/50, Loss: 0.377739\n",
      "Epoch 42/50, Loss: 0.377102\n",
      "Epoch 43/50, Loss: 0.376549\n",
      "Epoch 44/50, Loss: 0.376005\n",
      "Epoch 45/50, Loss: 0.375463\n",
      "Epoch 46/50, Loss: 0.374936\n",
      "Epoch 47/50, Loss: 0.374421\n",
      "Epoch 48/50, Loss: 0.373960\n",
      "Epoch 49/50, Loss: 0.373477\n",
      "Epoch 50/50, Loss: 0.373023\n",
      "Optimizer: mini-batch-gd, Accuracy: 0.3000\n",
      "Epoch 1/50, Loss: 0.427833\n",
      "Epoch 2/50, Loss: 0.421542\n",
      "Epoch 3/50, Loss: 0.413113\n",
      "Epoch 4/50, Loss: 0.403315\n",
      "Epoch 5/50, Loss: 0.392906\n",
      "Epoch 6/50, Loss: 0.382584\n",
      "Epoch 7/50, Loss: 0.372942\n",
      "Epoch 8/50, Loss: 0.364442\n",
      "Epoch 9/50, Loss: 0.357397\n",
      "Epoch 10/50, Loss: 0.351962\n",
      "Epoch 11/50, Loss: 0.348139\n",
      "Epoch 12/50, Loss: 0.345791\n",
      "Epoch 13/50, Loss: 0.344664\n",
      "Epoch 14/50, Loss: 0.344427\n",
      "Epoch 15/50, Loss: 0.344717\n",
      "Epoch 16/50, Loss: 0.345177\n",
      "Epoch 17/50, Loss: 0.345502\n",
      "Epoch 18/50, Loss: 0.345461\n",
      "Epoch 19/50, Loss: 0.344914\n",
      "Epoch 20/50, Loss: 0.343811\n",
      "Epoch 21/50, Loss: 0.342185\n",
      "Epoch 22/50, Loss: 0.340131\n",
      "Epoch 23/50, Loss: 0.337782\n",
      "Epoch 24/50, Loss: 0.335292\n",
      "Epoch 25/50, Loss: 0.332809\n",
      "Epoch 26/50, Loss: 0.330458\n",
      "Epoch 27/50, Loss: 0.328333\n",
      "Epoch 28/50, Loss: 0.326489\n",
      "Epoch 29/50, Loss: 0.324942\n",
      "Epoch 30/50, Loss: 0.323675\n",
      "Epoch 31/50, Loss: 0.322646\n",
      "Epoch 32/50, Loss: 0.321798\n",
      "Epoch 33/50, Loss: 0.321068\n",
      "Epoch 34/50, Loss: 0.320393\n",
      "Epoch 35/50, Loss: 0.319719\n",
      "Epoch 36/50, Loss: 0.319005\n",
      "Epoch 37/50, Loss: 0.318223\n",
      "Epoch 38/50, Loss: 0.317358\n",
      "Epoch 39/50, Loss: 0.316409\n",
      "Epoch 40/50, Loss: 0.315385\n",
      "Epoch 41/50, Loss: 0.314299\n",
      "Epoch 42/50, Loss: 0.313170\n",
      "Epoch 43/50, Loss: 0.312020\n",
      "Epoch 44/50, Loss: 0.310866\n",
      "Epoch 45/50, Loss: 0.309725\n",
      "Epoch 46/50, Loss: 0.308610\n",
      "Epoch 47/50, Loss: 0.307529\n",
      "Epoch 48/50, Loss: 0.306484\n",
      "Epoch 49/50, Loss: 0.305476\n",
      "Epoch 50/50, Loss: 0.304502\n",
      "Optimizer: momentum, Accuracy: 0.7000\n",
      "Epoch 1/50, Loss: 0.578924\n",
      "Epoch 2/50, Loss: 0.566106\n",
      "Epoch 3/50, Loss: 0.549862\n",
      "Epoch 4/50, Loss: 0.530692\n",
      "Epoch 5/50, Loss: 0.509264\n",
      "Epoch 6/50, Loss: 0.486509\n",
      "Epoch 7/50, Loss: 0.463631\n",
      "Epoch 8/50, Loss: 0.441926\n",
      "Epoch 9/50, Loss: 0.422447\n",
      "Epoch 10/50, Loss: 0.405730\n",
      "Epoch 11/50, Loss: 0.391797\n",
      "Epoch 12/50, Loss: 0.380361\n",
      "Epoch 13/50, Loss: 0.371017\n",
      "Epoch 14/50, Loss: 0.363363\n",
      "Epoch 15/50, Loss: 0.357037\n",
      "Epoch 16/50, Loss: 0.351728\n",
      "Epoch 17/50, Loss: 0.347168\n",
      "Epoch 18/50, Loss: 0.343128\n",
      "Epoch 19/50, Loss: 0.339411\n",
      "Epoch 20/50, Loss: 0.335850\n",
      "Epoch 21/50, Loss: 0.332308\n",
      "Epoch 22/50, Loss: 0.328681\n",
      "Epoch 23/50, Loss: 0.324897\n",
      "Epoch 24/50, Loss: 0.320925\n",
      "Epoch 25/50, Loss: 0.316775\n",
      "Epoch 26/50, Loss: 0.312492\n",
      "Epoch 27/50, Loss: 0.308145\n",
      "Epoch 28/50, Loss: 0.303809\n",
      "Epoch 29/50, Loss: 0.299549\n",
      "Epoch 30/50, Loss: 0.295405\n",
      "Epoch 31/50, Loss: 0.291388\n",
      "Epoch 32/50, Loss: 0.287493\n",
      "Epoch 33/50, Loss: 0.283703\n",
      "Epoch 34/50, Loss: 0.279997\n",
      "Epoch 35/50, Loss: 0.276358\n",
      "Epoch 36/50, Loss: 0.272772\n",
      "Epoch 37/50, Loss: 0.269234\n",
      "Epoch 38/50, Loss: 0.265742\n",
      "Epoch 39/50, Loss: 0.262303\n",
      "Epoch 40/50, Loss: 0.258932\n",
      "Epoch 41/50, Loss: 0.255646\n",
      "Epoch 42/50, Loss: 0.252466\n",
      "Epoch 43/50, Loss: 0.249415\n",
      "Epoch 44/50, Loss: 0.246513\n",
      "Epoch 45/50, Loss: 0.243777\n",
      "Epoch 46/50, Loss: 0.241220\n",
      "Epoch 47/50, Loss: 0.238849\n",
      "Epoch 48/50, Loss: 0.236664\n",
      "Epoch 49/50, Loss: 0.234659\n",
      "Epoch 50/50, Loss: 0.232826\n",
      "Optimizer: nesterov, Accuracy: 0.6333\n",
      "Epoch 1/50, Loss: 1.238788\n",
      "Epoch 2/50, Loss: 1.217361\n",
      "Epoch 3/50, Loss: 1.199093\n",
      "Epoch 4/50, Loss: 1.182686\n",
      "Epoch 5/50, Loss: 1.167532\n",
      "Epoch 6/50, Loss: 1.153288\n",
      "Epoch 7/50, Loss: 1.139740\n",
      "Epoch 8/50, Loss: 1.126744\n",
      "Epoch 9/50, Loss: 1.114197\n",
      "Epoch 10/50, Loss: 1.102025\n",
      "Epoch 11/50, Loss: 1.090172\n",
      "Epoch 12/50, Loss: 1.078592\n",
      "Epoch 13/50, Loss: 1.067252\n",
      "Epoch 14/50, Loss: 1.056122\n",
      "Epoch 15/50, Loss: 1.045182\n",
      "Epoch 16/50, Loss: 1.034411\n",
      "Epoch 17/50, Loss: 1.023796\n",
      "Epoch 18/50, Loss: 1.013324\n",
      "Epoch 19/50, Loss: 1.002984\n",
      "Epoch 20/50, Loss: 0.992767\n",
      "Epoch 21/50, Loss: 0.982668\n",
      "Epoch 22/50, Loss: 0.972681\n",
      "Epoch 23/50, Loss: 0.962801\n",
      "Epoch 24/50, Loss: 0.953024\n",
      "Epoch 25/50, Loss: 0.943348\n",
      "Epoch 26/50, Loss: 0.933771\n",
      "Epoch 27/50, Loss: 0.924292\n",
      "Epoch 28/50, Loss: 0.914909\n",
      "Epoch 29/50, Loss: 0.905622\n",
      "Epoch 30/50, Loss: 0.896430\n",
      "Epoch 31/50, Loss: 0.887334\n",
      "Epoch 32/50, Loss: 0.878333\n",
      "Epoch 33/50, Loss: 0.869428\n",
      "Epoch 34/50, Loss: 0.860618\n",
      "Epoch 35/50, Loss: 0.851906\n",
      "Epoch 36/50, Loss: 0.843290\n",
      "Epoch 37/50, Loss: 0.834771\n",
      "Epoch 38/50, Loss: 0.826350\n",
      "Epoch 39/50, Loss: 0.818028\n",
      "Epoch 40/50, Loss: 0.809805\n",
      "Epoch 41/50, Loss: 0.801681\n",
      "Epoch 42/50, Loss: 0.793657\n",
      "Epoch 43/50, Loss: 0.785733\n",
      "Epoch 44/50, Loss: 0.777910\n",
      "Epoch 45/50, Loss: 0.770188\n",
      "Epoch 46/50, Loss: 0.762568\n",
      "Epoch 47/50, Loss: 0.755050\n",
      "Epoch 48/50, Loss: 0.747634\n",
      "Epoch 49/50, Loss: 0.740320\n",
      "Epoch 50/50, Loss: 0.733109\n",
      "Optimizer: adagrad, Accuracy: 0.2000\n",
      "Epoch 1/50, Loss: 0.358754\n",
      "Epoch 2/50, Loss: 0.341547\n",
      "Epoch 3/50, Loss: 0.330318\n",
      "Epoch 4/50, Loss: 0.321929\n",
      "Epoch 5/50, Loss: 0.314983\n",
      "Epoch 6/50, Loss: 0.308735\n",
      "Epoch 7/50, Loss: 0.302730\n",
      "Epoch 8/50, Loss: 0.296672\n",
      "Epoch 9/50, Loss: 0.290344\n",
      "Epoch 10/50, Loss: 0.283595\n",
      "Epoch 11/50, Loss: 0.276357\n",
      "Epoch 12/50, Loss: 0.268665\n",
      "Epoch 13/50, Loss: 0.260660\n",
      "Epoch 14/50, Loss: 0.252550\n",
      "Epoch 15/50, Loss: 0.244564\n",
      "Epoch 16/50, Loss: 0.236901\n",
      "Epoch 17/50, Loss: 0.229702\n",
      "Epoch 18/50, Loss: 0.223037\n",
      "Epoch 19/50, Loss: 0.216916\n",
      "Epoch 20/50, Loss: 0.211308\n",
      "Epoch 21/50, Loss: 0.206161\n",
      "Epoch 22/50, Loss: 0.201416\n",
      "Epoch 23/50, Loss: 0.197016\n",
      "Epoch 24/50, Loss: 0.192908\n",
      "Epoch 25/50, Loss: 0.189045\n",
      "Epoch 26/50, Loss: 0.185388\n",
      "Epoch 27/50, Loss: 0.181905\n",
      "Epoch 28/50, Loss: 0.178569\n",
      "Epoch 29/50, Loss: 0.175355\n",
      "Epoch 30/50, Loss: 0.172247\n",
      "Epoch 31/50, Loss: 0.169229\n",
      "Epoch 32/50, Loss: 0.166293\n",
      "Epoch 33/50, Loss: 0.163433\n",
      "Epoch 34/50, Loss: 0.160645\n",
      "Epoch 35/50, Loss: 0.157928\n",
      "Epoch 36/50, Loss: 0.155282\n",
      "Epoch 37/50, Loss: 0.152705\n",
      "Epoch 38/50, Loss: 0.150195\n",
      "Epoch 39/50, Loss: 0.147750\n",
      "Epoch 40/50, Loss: 0.145368\n",
      "Epoch 41/50, Loss: 0.143047\n",
      "Epoch 42/50, Loss: 0.140786\n",
      "Epoch 43/50, Loss: 0.138583\n",
      "Epoch 44/50, Loss: 0.136438\n",
      "Epoch 45/50, Loss: 0.134349\n",
      "Epoch 46/50, Loss: 0.132316\n",
      "Epoch 47/50, Loss: 0.130336\n",
      "Epoch 48/50, Loss: 0.128408\n",
      "Epoch 49/50, Loss: 0.126530\n",
      "Epoch 50/50, Loss: 0.124700\n",
      "Optimizer: rmsprop, Accuracy: 0.9667\n",
      "Epoch 1/50, Loss: 0.471110\n",
      "Epoch 2/50, Loss: 0.455478\n",
      "Epoch 3/50, Loss: 0.440541\n",
      "Epoch 4/50, Loss: 0.426202\n",
      "Epoch 5/50, Loss: 0.412430\n",
      "Epoch 6/50, Loss: 0.399176\n",
      "Epoch 7/50, Loss: 0.386368\n",
      "Epoch 8/50, Loss: 0.373944\n",
      "Epoch 9/50, Loss: 0.361870\n",
      "Epoch 10/50, Loss: 0.350143\n",
      "Epoch 11/50, Loss: 0.338780\n",
      "Epoch 12/50, Loss: 0.327809\n",
      "Epoch 13/50, Loss: 0.317257\n",
      "Epoch 14/50, Loss: 0.307150\n",
      "Epoch 15/50, Loss: 0.297506\n",
      "Epoch 16/50, Loss: 0.288339\n",
      "Epoch 17/50, Loss: 0.279655\n",
      "Epoch 18/50, Loss: 0.271456\n",
      "Epoch 19/50, Loss: 0.263735\n",
      "Epoch 20/50, Loss: 0.256482\n",
      "Epoch 21/50, Loss: 0.249679\n",
      "Epoch 22/50, Loss: 0.243303\n",
      "Epoch 23/50, Loss: 0.237329\n",
      "Epoch 24/50, Loss: 0.231729\n",
      "Epoch 25/50, Loss: 0.226477\n",
      "Epoch 26/50, Loss: 0.221552\n",
      "Epoch 27/50, Loss: 0.216931\n",
      "Epoch 28/50, Loss: 0.212599\n",
      "Epoch 29/50, Loss: 0.208541\n",
      "Epoch 30/50, Loss: 0.204745\n",
      "Epoch 31/50, Loss: 0.201197\n",
      "Epoch 32/50, Loss: 0.197887\n",
      "Epoch 33/50, Loss: 0.194803\n",
      "Epoch 34/50, Loss: 0.191936\n",
      "Epoch 35/50, Loss: 0.189276\n",
      "Epoch 36/50, Loss: 0.186815\n",
      "Epoch 37/50, Loss: 0.184545\n",
      "Epoch 38/50, Loss: 0.182457\n",
      "Epoch 39/50, Loss: 0.180537\n",
      "Epoch 40/50, Loss: 0.178769\n",
      "Epoch 41/50, Loss: 0.177132\n",
      "Epoch 42/50, Loss: 0.175605\n",
      "Epoch 43/50, Loss: 0.174164\n",
      "Epoch 44/50, Loss: 0.172790\n",
      "Epoch 45/50, Loss: 0.171469\n",
      "Epoch 46/50, Loss: 0.170188\n",
      "Epoch 47/50, Loss: 0.168938\n",
      "Epoch 48/50, Loss: 0.167712\n",
      "Epoch 49/50, Loss: 0.166505\n",
      "Epoch 50/50, Loss: 0.165314\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: adam, Accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FeedForwardNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return -np.mean(y_true * np.log(y_pred))\n",
    "\n",
    "    def compute_gradients(self, X, y_true):\n",
    "        m = X.shape[0]\n",
    "        y_pred = self.forward(X)\n",
    "        dZ2 = y_pred - y_true\n",
    "        dW2 = np.dot(self.a1.T, dZ2) / m\n",
    "        db2 = np.sum(dZ2, axis=0) / m\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * self.a1 * (1 - self.a1)\n",
    "        dW1 = np.dot(X.T, dZ1) / m\n",
    "        db1 = np.sum(dZ1, axis=0) / m\n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def fit(self, X, y, optimizer='gd', learning_rate=0.01, epochs=100, batch_size=32, gamma=0.9, epsilon=1e-8, beta1=0.9, beta2=0.999):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        if optimizer not in ['gd', 'mini-batch-gd', 'momentum', 'nesterov', 'adagrad', 'rmsprop', 'adam']:\n",
    "            raise ValueError(\"Invalid optimizer specified.\")\n",
    "\n",
    "        if optimizer in ['momentum', 'nesterov']:\n",
    "            self.velocity_W1 = np.zeros_like(self.W1)\n",
    "            self.velocity_b1 = np.zeros_like(self.b1)\n",
    "            self.velocity_W2 = np.zeros_like(self.W2)\n",
    "            self.velocity_b2 = np.zeros_like(self.b2)\n",
    "\n",
    "        if optimizer == 'adagrad':\n",
    "            self.cache_W1 = np.zeros_like(self.W1)\n",
    "            self.cache_b1 = np.zeros_like(self.b1)\n",
    "            self.cache_W2 = np.zeros_like(self.W2)\n",
    "            self.cache_b2 = np.zeros_like(self.b2)\n",
    "\n",
    "        if optimizer == 'rmsprop':\n",
    "            self.cache_W1 = np.zeros_like(self.W1)\n",
    "            self.cache_b1 = np.zeros_like(self.b1)\n",
    "            self.cache_W2 = np.zeros_like(self.W2)\n",
    "            self.cache_b2 = np.zeros_like(self.b2)\n",
    "\n",
    "        if optimizer == 'adam':\n",
    "            self.m_W1 = np.zeros_like(self.W1)\n",
    "            self.m_b1 = np.zeros_like(self.b1)\n",
    "            self.m_W2 = np.zeros_like(self.W2)\n",
    "            self.m_b2 = np.zeros_like(self.b2)\n",
    "            self.v_W1 = np.zeros_like(self.W1)\n",
    "            self.v_b1 = np.zeros_like(self.b1)\n",
    "            self.v_W2 = np.zeros_like(self.W2)\n",
    "            self.v_b2 = np.zeros_like(self.b2)\n",
    "            self.t = 0\n",
    "            self.losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if optimizer == 'mini-batch-gd':\n",
    "                indices = np.random.permutation(m)\n",
    "                for i in range(0, m, batch_size):\n",
    "                    batch_indices = indices[i:i+batch_size]\n",
    "                    X_batch, y_batch = X[batch_indices], y[batch_indices]\n",
    "                    dW1, db1, dW2, db2 = self.compute_gradients(X_batch, y_batch)\n",
    "                    self.W1 -= learning_rate * dW1\n",
    "                    self.b1 -= learning_rate * db1\n",
    "                    self.W2 -= learning_rate * dW2\n",
    "                    self.b2 -= learning_rate * db2\n",
    "            else:\n",
    "                dW1, db1, dW2, db2 = self.compute_gradients(X, y)\n",
    "                if optimizer == 'gd':\n",
    "                    self.W1 -= learning_rate * dW1\n",
    "                    self.b1 -= learning_rate * db1\n",
    "                    self.W2 -= learning_rate * dW2\n",
    "                    self.b2 -= learning_rate * db2\n",
    "                elif optimizer == 'momentum':\n",
    "                    self.velocity_W1 = gamma * self.velocity_W1 + learning_rate * dW1\n",
    "                    self.velocity_b1 = gamma * self.velocity_b1 + learning_rate * db1\n",
    "                    self.velocity_W2 = gamma * self.velocity_W2 + learning_rate * dW2\n",
    "                    self.velocity_b2 = gamma * self.velocity_b2 + learning_rate * db2\n",
    "                    self.W1 -= self.velocity_W1\n",
    "                    self.b1 -= self.velocity_b1\n",
    "                    self.W2 -= self.velocity_W2\n",
    "                    self.b2 -= self.velocity_b2\n",
    "                elif optimizer == 'nesterov':\n",
    "                    self.velocity_W1 = gamma * self.velocity_W1 + learning_rate * dW1\n",
    "                    self.velocity_b1 = gamma * self.velocity_b1 + learning_rate * db1\n",
    "                    self.velocity_W2 = gamma * self.velocity_W2 + learning_rate * dW2\n",
    "                    self.velocity_b2 = gamma * self.velocity_b2 + learning_rate * db2\n",
    "                    self.W1 -= gamma * self.velocity_W1 + learning_rate * dW1\n",
    "                    self.b1 -= gamma * self.velocity_b1 + learning_rate * db1\n",
    "                    self.W2 -= gamma * self.velocity_W2 + learning_rate * dW2\n",
    "                    self.b2 -= gamma * self.velocity_b2 + learning_rate * db2\n",
    "                elif optimizer == 'adagrad':\n",
    "                    self.cache_W1 += dW1**2\n",
    "                    self.cache_b1 += db1**2\n",
    "                    self.cache_W2 += dW2**2\n",
    "                    self.cache_b2 += db2**2\n",
    "                    self.W1 -= learning_rate * dW1 / (np.sqrt(self.cache_W1) + epsilon)\n",
    "                    self.b1 -= learning_rate * db1 / (np.sqrt(self.cache_b1) + epsilon)\n",
    "                    self.W2 -= learning_rate * dW2 / (np.sqrt(self.cache_W2) + epsilon)\n",
    "                    self.b2 -= learning_rate * db2 / (np.sqrt(self.cache_b2) + epsilon)\n",
    "                elif optimizer == 'rmsprop':\n",
    "                    self.cache_W1 = gamma * self.cache_W1 + (1 - gamma) * dW1**2\n",
    "                    self.cache_b1 = gamma * self.cache_b1 + (1 - gamma) * db1**2\n",
    "                    self.cache_W2 = gamma * self.cache_W2 + (1 - gamma) * dW2**2\n",
    "                    self.cache_b2 = gamma * self.cache_b2 + (1 - gamma) * db2**2\n",
    "                    self.W1 -= learning_rate * dW1 / (np.sqrt(self.cache_W1) + epsilon)\n",
    "                    self.b1 -= learning_rate * db1 / (np.sqrt(self.cache_b1) + epsilon)\n",
    "                    self.W2 -= learning_rate * dW2 / (np.sqrt(self.cache_W2) + epsilon)\n",
    "                    self.b2 -= learning_rate * db2 / (np.sqrt(self.cache_b2) + epsilon)\n",
    "                elif optimizer == 'adam':\n",
    "                    self.t += 1\n",
    "                    self.m_W1 = beta1 * self.m_W1 + (1 - beta1) * dW1\n",
    "                    self.m_b1 = beta1 * self.m_b1 + (1 - beta1) * db1\n",
    "                    self.m_W2 = beta1 * self.m_W2 + (1 - beta1) * dW2\n",
    "                    self.m_b2 = beta1 * self.m_b2 + (1 - beta1) * db2\n",
    "                    self.v_W1 = beta2 * self.v_W1 + (1 - beta2) * dW1**2\n",
    "                    self.v_b1 = beta2 * self.v_b1 + (1 - beta2) * db1**2\n",
    "                    self.v_W2 = beta2 * self.v_W2 + (1 - beta2) * dW2**2\n",
    "                    self.v_b2 = beta2 * self.v_b2 + (1 - beta2) * db2**2\n",
    "                    m_W1_hat = self.m_W1 / (1 - beta1**self.t)\n",
    "                    m_b1_hat = self.m_b1 / (1 - beta1**self.t)\n",
    "                    m_W2_hat = self.m_W2 / (1 - beta1**self.t)\n",
    "                    m_b2_hat = self.m_b2 / (1 - beta1**self.t)\n",
    "                    v_W1_hat = self.v_W1 / (1 - beta2**self.t)\n",
    "                    v_b1_hat = self.v_b1 / (1 - beta2**self.t)\n",
    "                    v_W2_hat = self.v_W2 / (1 - beta2**self.t)\n",
    "                    v_b2_hat = self.v_b2 / (1 - beta2**self.t)\n",
    "                    self.W1 -= learning_rate * m_W1_hat / (np.sqrt(v_W1_hat) + epsilon)\n",
    "                    self.b1 -= learning_rate * m_b1_hat / (np.sqrt(v_b1_hat) + epsilon)\n",
    "                    self.W2 -= learning_rate * m_W2_hat / (np.sqrt(v_W2_hat) + epsilon)\n",
    "                    self.b2 -= learning_rate * m_b2_hat / (np.sqrt(v_b2_hat) + epsilon)\n",
    "\n",
    "            loss = self.compute_loss(y, self.forward(X))\n",
    "            if optimizer == 'adam':\n",
    "                self.losses.append(loss)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}\")\n",
    "\n",
    "    def plot_loss(self):\n",
    "        if hasattr(self, 'losses'):\n",
    "            plt.plot(range(1, len(self.losses) + 1), self.losses)\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Loss vs. Epochs')\n",
    "            plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# One-hot encode the target labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training the FeedForward Neural Network using various optimizers\n",
    "optimizers = ['gd', 'mini-batch-gd', 'momentum', 'nesterov', 'adagrad', 'rmsprop', 'adam']\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    nn = FeedForwardNeuralNetwork(input_size=4, hidden_size=10, output_size=3)\n",
    "    nn.fit(X_train, y_train, optimizer=optimizer, learning_rate=0.01, epochs=50)\n",
    "    nn.plot_loss()\n",
    "    y_pred = nn.predict(X_test)\n",
    "    accuracy = np.mean(y_pred == np.argmax(y_test, axis=1))\n",
    "    print(f\"Optimizer: {optimizer}, Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9fb6d-eac9-4d7c-a71c-edb21ad3d5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
